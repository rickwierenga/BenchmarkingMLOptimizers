{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AnJO_odC_tTD"
   },
   "source": [
    "# Benchmarking Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmdeHcS7_tTF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "sn3NLeWB_tTJ",
    "outputId": "0677ca79-f398-48dd-9beb-1f2527c601aa"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiuIf--5Snma"
   },
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFX7-C2PYmRX"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "x_train = keras.utils.normalize(x_train, axis=1)\n",
    "x_test = keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89pbY3g9Hr6x"
   },
   "outputs": [],
   "source": [
    "def train_mnist(optimizer):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=x_train[0].shape),\n",
    "        keras.layers.Dense(250, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=False,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "YmjHQVMDKQ1N",
    "outputId": "025f39af-add3-444e-991a-7793141bed28"
   },
   "outputs": [],
   "source": [
    "mnist = {\n",
    "  'sgd': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []},\n",
    "  'rmsprop': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []},\n",
    "  'adagrad': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []},\n",
    "  'adam': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []}\n",
    "}\n",
    "for optimizer in ['sgd', 'rmsprop', 'adagrad', 'adam']:\n",
    "    print('--- {} ---'.format(optimizer))\n",
    "    for i in range(10):\n",
    "        print('starting ', i)\n",
    "        history, model = train_mnist(optimizer)\n",
    "        train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "        val_loss, val_accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "        mnist[optimizer]['loss'].append(train_loss)\n",
    "        mnist[optimizer]['acc'].append(train_accuracy)\n",
    "        mnist[optimizer]['val_loss'].append(val_loss)\n",
    "        mnist[optimizer]['val_acc'].append(val_accuracy)\n",
    "        mnist[optimizer]['history'].append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "WSK9Z7-FN77e",
    "outputId": "940d2950-1ca5-43d6-f461-8f4f89e59954"
   },
   "outputs": [],
   "source": [
    "for opt in ['sgd', 'rmsprop', 'adagrad', 'adam']:\n",
    "    for key in ['acc', 'val_acc', 'loss', 'val_loss']:\n",
    "        print(opt, key, np.mean(mnist[opt][key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "J0jfpfJgWGVi",
    "outputId": "54f13875-20f0-49b8-e2e0-f97c8b06521b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    key = ['acc', 'val_acc', 'loss', 'val_loss'][i]\n",
    "    title = ['Training Accuracy on MNIST', 'Validation Accuracy on MNIST', 'Training Loss on MNIST', 'Validation Loss on MNIST'][i]\n",
    "    for opt in ['sgd', 'rmsprop', 'adagrad', 'adam']:\n",
    "        hist = np.zeros(10)\n",
    "        for h in mnist[opt]['history']:\n",
    "              hist += np.array(h.history[key])\n",
    "        mean = hist / 10\n",
    "        plt.plot(mean, label=opt)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGw-t7VH_tTd"
   },
   "source": [
    "## CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Vlzij8Hn_tTe",
    "outputId": "bd4398f3-f2af-4fce-ada9-8ad9247dead3"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "x_train = keras.utils.normalize(x_train, axis=1)\n",
    "x_test = keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HCfl_rIFqHg"
   },
   "outputs": [],
   "source": [
    "def train_cifar10(optimizer):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, kernel_size=(2, 2), padding='same', activation='relu', input_shape=x_train[0].shape),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "\n",
    "        keras.layers.Conv2D(16, kernel_size=(2, 2), padding='same', activation='relu'),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        keras.layers.Dropout(0.3),\n",
    "\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(1024, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.categorical_crossentropy,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                      batch_size=128,\n",
    "                      epochs=30,\n",
    "                      shuffle=True,\n",
    "                      verbose=False,\n",
    "                      validation_data=(x_test, y_test))\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WhMHuvHJPkAq",
    "outputId": "2ae6c9c2-d715-4ab6-d7aa-6047602be395"
   },
   "outputs": [],
   "source": [
    "cifar10 = {\n",
    "    'sgd': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []},\n",
    "    'rmsprop': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []},\n",
    "    'adagrad': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []},\n",
    "    'adam': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []}\n",
    "  }\n",
    "for optimizer in ['sgd', 'rmsprop', 'adagrad', 'adam']:\n",
    "    print('--- {} ---'.format(optimizer))\n",
    "    for i in range(10):\n",
    "        print('starting ', i)\n",
    "        history, model = train_cifar10(optimizer)\n",
    "        train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "        val_loss, val_accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "        cifar10[optimizer]['loss'].append(train_loss)\n",
    "        cifar10[optimizer]['acc'].append(train_accuracy)\n",
    "        cifar10[optimizer]['val_loss'].append(val_loss)\n",
    "        cifar10[optimizer]['val_acc'].append(val_accuracy)\n",
    "        cifar10[optimizer]['history'].append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "tgSuUG2bVb6N",
    "outputId": "74f6c234-5b1f-460a-d6f2-4c0e011ce3cb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    key = ['acc', 'val_acc', 'loss', 'val_loss'][i]\n",
    "    title = ['Training Accuracy on CIFAR10', 'Validation Accuracy on CIFAR10', 'Training Loss on CIFAR10', 'Validation Loss on CIFAR10'][i]\n",
    "    for opt in ['sgd', 'rmsprop', 'adagrad', 'adam']:\n",
    "        hist = np.zeros(30)\n",
    "        for h in cifar10[opt]['history']:\n",
    "          hist += np.array(h.history[key])\n",
    "        mean = hist / 10\n",
    "        plt.plot(mean, label=opt)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "5oa5jELKVy_V",
    "outputId": "0edc15ea-1868-422f-a36f-840ec6b09b38"
   },
   "outputs": [],
   "source": [
    "for opt in ['sgd', 'rmsprop', 'adagrad', 'adam']:\n",
    "    for key in ['acc', 'val_acc', 'loss', 'val_loss']:\n",
    "        print(opt, key, np.mean(cifar10[opt][key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1iLDCSnvYw79"
   },
   "source": [
    "## Text classification with preprocessed text: Movie reviews (8k)\n",
    "\n",
    "https://www.tensorflow.org/tutorials/keras/text_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "mtBRpur4ZE7E",
    "outputId": "f81f71da-31e0-4dc8-a510-86845deaccd8"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "(train_data, test_data), info = tfds.load(\n",
    "    'imdb_reviews/subwords8k',\n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    as_supervised=True,\n",
    "    with_info=True)\n",
    "\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_batches = (\n",
    "    train_data\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .padded_batch(32, train_data.output_shapes))\n",
    "\n",
    "test_batches = (\n",
    "    test_data\n",
    "    .padded_batch(32, train_data.output_shapes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyVM0CcmZOIV"
   },
   "outputs": [],
   "source": [
    "def train_imdb(optimizer):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Embedding(encoder.vocab_size, 16),\n",
    "        keras.layers.GlobalAveragePooling1D(),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_batches,\n",
    "                        epochs=10,\n",
    "                        verbose=False,\n",
    "                        shuffle=True,\n",
    "                        validation_data=test_batches,\n",
    "                        validation_steps=30)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MmP1PO_0aNnY",
    "outputId": "684b54bb-b779-4ac4-f7c9-9efea31cbfa2"
   },
   "outputs": [],
   "source": [
    "imdb = {\n",
    "  'sgd': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []},\n",
    "  'rmsprop': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []},\n",
    "  'adagrad': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []},\n",
    "  'adam': {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': [], 'history': []}\n",
    "}\n",
    "for optimizer in ['sgd', 'rmsprop', 'adagrad', 'adam']:\n",
    "    print('--- {} ---'.format(optimizer))\n",
    "    for i in range(10):\n",
    "        print('starting ', i)\n",
    "        history, model = train_imdb(optimizer)\n",
    "        train_loss, train_accuracy = model.evaluate(train_batches, verbose=False)\n",
    "        val_loss, val_accuracy = model.evaluate(test_batches, verbose=False)\n",
    "        imdb[optimizer]['loss'].append(train_loss)\n",
    "        imdb[optimizer]['acc'].append(train_accuracy)\n",
    "        imdb[optimizer]['val_loss'].append(val_loss)\n",
    "        imdb[optimizer]['val_acc'].append(val_accuracy)\n",
    "        imdb[optimizer]['history'].append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "ddAl8Souakfl",
    "outputId": "279ce7d2-48ed-453b-9a46-92a840d5dd8d"
   },
   "outputs": [],
   "source": [
    "for opt in ['sgd', 'rmsprop', 'adagrad', 'adam']:\n",
    "    for key in ['acc', 'val_acc', 'loss', 'val_loss']:\n",
    "        print(opt, key, np.mean(imdb[opt][key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "zAxlddxnara0",
    "outputId": "b366ad6f-2f7c-4733-9be5-fa2048117387"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    key = ['acc', 'val_acc', 'loss', 'val_loss'][i]\n",
    "    title = ['Training Accuracy on IMDB 8K', 'Validation Accuracy on IMDB 8K', 'Training Loss on IMDB 8K', 'Validation Loss on IMDB 8K'][i]\n",
    "    for opt in ['sgd', 'rmsprop', 'adagrad', 'adam']:\n",
    "        hist = np.zeros(10)\n",
    "        for h in imdb[opt]['history']:\n",
    "            hist += np.array(h.history[key])\n",
    "        mean = hist / 10\n",
    "        plt.plot(mean, label=opt)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eiuIf--5Snma",
    "EGw-t7VH_tTd",
    "1iLDCSnvYw79"
   ],
   "name": "Benchmarking Optimizers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
